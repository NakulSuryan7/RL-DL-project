{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8335,"status":"ok","timestamp":1683451477535,"user":{"displayName":"GOVIND M B","userId":"08292525403380205782"},"user_tz":-330},"id":"c9M_NCSKJWX8"},"outputs":[],"source":["import random\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683451477536,"user":{"displayName":"GOVIND M B","userId":"08292525403380205782"},"user_tz":-330},"id":"c5kGD6MmJwIG"},"outputs":[],"source":["\n","__all__ = ['relationship_learning', 'direct_relationship_learning']\n","\n","\n","def calibrate(logits, labels):\n","    \"\"\"\n","    calibrate by minimizing negative log likelihood.\n","    :param logits: pytorch tensor with shape of [N, N_c]\n","    :param labels: pytorch tensor of labels\n","    :return: float\n","    \"\"\"\n","    scale = nn.Parameter(torch.ones(\n","        1, 1, dtype=torch.float32), requires_grad=True)\n","    optim = torch.optim.LBFGS([scale])\n","\n","    def loss():\n","        optim.zero_grad()\n","        lo = nn.CrossEntropyLoss()(logits * scale, labels)\n","        lo.backward()\n","        return lo\n","\n","    state = optim.state[scale]\n","    for i in range(20):\n","        optim.step(loss)\n","        print(f'calibrating, {scale.item()}')\n","        if state['n_iter'] < optim.state_dict()['param_groups'][0]['max_iter']:\n","            break\n","\n","    return scale.item()\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":884,"status":"ok","timestamp":1683451478417,"user":{"displayName":"GOVIND M B","userId":"08292525403380205782"},"user_tz":-330},"id":"YPVji7CnJ6C7"},"outputs":[],"source":["# def softmax_np(x):\n","#     max_el = np.max(x, axis=1, keepdims=True)\n","#     x = x - max_el\n","#     x = np.exp(x)\n","#     s = np.sum(x, axis=1, keepdims=True)\n","#     return x / s\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683451478417,"user":{"displayName":"GOVIND M B","userId":"08292525403380205782"},"user_tz":-330},"id":"tAzT4z-iJ6S4"},"outputs":[],"source":["def relationship_learning(train_logits, train_labels, validation_logits, validation_labels):\n","    \"\"\"\n","\n","    :param train_logits (ImageNet logits): [N, N_p], where N_p is the number of classes in pre-trained dataset\n","    :param train_labels:  [N], where 0 <= each number < N_t, and N_t is the number of target dataset\n","    :param validation_logits (ImageNet logits): [N, N_p]\n","    :param validation_labels:  [N]\n","    :return: [N_c, N_p] matrix representing the conditional probability p(pre-trained class | target_class)\n","     \"\"\"\n","\n","    # convert logits to probabilities\n","    train_probabilities = softmax_np(train_logits * 0.8840456604957581)\n","    validation_probabilities = softmax_np(\n","        validation_logits * 0.8840456604957581)\n","\n","    all_probabilities = np.concatenate(\n","        (train_probabilities, validation_probabilities))\n","    all_labels = np.concatenate((train_labels, validation_labels))\n","\n","    Cs = []\n","    accs = []\n","    classifiers = []\n","    for C in [1e4, 3e3, 1e3, 3e2, 1e2, 3e1, 1e1, 3.0, 1.0, 3e-1, 1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]:\n","        cls = LogisticRegression(\n","            multi_class='multinomial', C=C, fit_intercept=False)\n","        cls.fit(train_probabilities, train_labels)\n","        val_predict = cls.predict(validation_probabilities)\n","        val_acc = np.sum((val_predict == validation_labels).astype(\n","            np.float)) / len(validation_labels)\n","        Cs.append(C)\n","        accs.append(val_acc)\n","        classifiers.append(cls)\n","\n","    accs = np.asarray(accs)\n","    ind = int(np.argmax(accs))\n","    cls = classifiers[ind]\n","    del classifiers\n","\n","    validation_logits = np.matmul(validation_probabilities, cls.coef_.T)\n","    validation_logits = torch.from_numpy(validation_logits.astype(np.float32))\n","    validation_labels = torch.from_numpy(validation_labels)\n","\n","    scale = calibrate(validation_logits, validation_labels)\n","\n","    p_target_given_pretrain = softmax_np(\n","        cls.coef_.T * scale)  # shape of [N_p, N_c], conditional probability p(target_class | pre-trained class)\n","\n","    # in the paper, both ys marginal and yt marginal are computed\n","    # here we only use ys marginal to make sure p_pretrain_given_target is a valid conditional probability\n","    # (make sure p_pretrain_given_target[i] sums up to 1)\n","    pretrain_marginal = np.mean(all_probabilities, axis=0).reshape(\n","        (-1, 1))  # shape of [N_p, 1]\n","    p_joint_distribution = (p_target_given_pretrain * pretrain_marginal).T\n","    p_pretrain_given_target = p_joint_distribution / \\\n","        np.sum(p_joint_distribution, axis=1, keepdims=True)\n","\n","    return p_pretrain_given_target\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683451478417,"user":{"displayName":"GOVIND M B","userId":"08292525403380205782"},"user_tz":-330},"id":"sUS3lX3zNiuf"},"outputs":[],"source":["def direct_relationship_learning(train_logits, train_labels, validation_logits, validation_labels):\n","    \"\"\"\n","    The direct approach of learning category relationship.\n","\n","    :param train_logits (ImageNet logits): [N, N_p], where N_p is the number of classes in pre-trained dataset\n","    :param train_labels:  [N], where 0 <= each number < N_t, and N_t is the number of target dataset\n","    :param validation_logits (ImageNet logits): [N, N_p]\n","    :param validation_labels:  [N]\n","    :return: [N_c, N_p] matrix representing the conditional probability p(pre-trained class | target_class)\n","     \"\"\"\n","    # convert logits to probabilities\n","    train_probabilities = softmax_np(train_logits * 0.8840456604957581)\n","    validation_probabilities = softmax_np(\n","        validation_logits * 0.8840456604957581)\n","\n","    all_probabilities = np.concatenate(\n","        (train_probabilities, validation_probabilities))\n","    all_labels = np.concatenate((train_labels, validation_labels))\n","\n","    N_t = np.max(all_labels) + 1 # the number of target classes\n","    conditional = []\n","    for i in range(N_t):\n","        this_class = all_probabilities[all_labels == i]\n","        average = np.mean(this_class, axis=0, keepdims=True)\n","        conditional.append(average)\n","    return np.concatenate(conditional)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPh5Toj1eVl7dtmqgWVpYOt","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
