{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N-4g2Lt-Vf2",
        "outputId": "bbbdc8a4-e2ca-4e3e-905b-58f7b1593057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXnNskbaTrQw"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/module'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGFoIx4Pyx0_",
        "outputId": "b2a9baa4-c46e-468a-b601-ecb4b01a5c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/CoTuning-main/module\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from backbone import ResNet50_F, ResNet50_C\n",
        "from relationship_learning import relationship_learning"
      ],
      "metadata": {
        "id": "7Ai2NdDayyVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbO3Md6XCBWw",
        "outputId": "19415cbe-1e61-4bb0-c8f9-766052a30a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transforms import get_transforms\n",
        "from tools import AccuracyMeter, TenCropsTest"
      ],
      "metadata": {
        "id": "vuLduWxHyy6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet50_F\n",
        "ResNet50_C\n",
        "relationship_learning\n",
        "get_transforms\n",
        "AccuracyMeter\n",
        "TenCropsTest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h32XBaJIzTaJ",
        "outputId": "9a8d282d-5219-4841-ea61-fee77e16fdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tools.TenCropsTest(loader, net)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_writer(log_dir):\n",
        "    return SummaryWriter(log_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "3DyUmyxbT4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gpu=0\n",
        "seed=2020\n",
        "batch_size=48\n",
        "total_iter=9050\n",
        "eval_iter=1000\n",
        "save_iter=9000\n",
        "print_iter=100\n",
        "\n",
        "# dataset\n",
        "data_path=\"/content/drive/MyDrive/Colab Notebooks/CoTuning-main/CUB_200_2011\"\n",
        "class_num=200\n",
        "num_workers=2\n",
        "\n",
        "# optimizer\n",
        "lr=1e-3\n",
        "gamma=0.1\n",
        "nesterov=True\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "\n",
        "    # experiment\n",
        "root='.'\n",
        "name='StochNorm'\n",
        "trade_off=2.3\n",
        "relationship_path='relationship.npy'\n",
        "save_dir=\"model\"\n",
        "visual_dir=\"visual\""
      ],
      "metadata": {
        "id": "hPriwhTM8edJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def str2list(v):\n",
        "    return v.split(',')\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
        "\n",
        "\n",
        "def get_data_loader():\n",
        "    data_transforms = get_transforms(resize_size=256, crop_size=224)\n",
        "\n",
        "    # build dataset\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        os.path.join(data_path, 'images'),\n",
        "        transform=data_transforms['train'])\n",
        "    determin_train_dataset = datasets.ImageFolder(\n",
        "        os.path.join(data_path, 'images'),\n",
        "        transform=data_transforms['val'])\n",
        "    val_dataset = datasets.ImageFolder(\n",
        "        os.path.join(data_path, 'images'),\n",
        "        transform=data_transforms['val'])\n",
        "    test_datasets = {\n",
        "        'test' + str(i):\n",
        "            datasets.ImageFolder(\n",
        "                os.path.join(data_path, 'images'),\n",
        "                transform=data_transforms[\"test\" + str(i)]\n",
        "        )\n",
        "        for i in range(10)\n",
        "    }\n",
        "\n",
        "    # build dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "    determin_train_loader = DataLoader(determin_train_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                       num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "    test_loaders = {\n",
        "        'test' + str(i):\n",
        "            DataLoader(\n",
        "                test_datasets[\"test\" + str(i)],\n",
        "                batch_size=4, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "        for i in range(10)\n",
        "    }\n",
        "\n",
        "    return train_loader, determin_train_loader, val_loader, test_loaders\n"
      ],
      "metadata": {
        "id": "n1tL9eieT41t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seeds(seed):\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def main():\n",
        "    torch.cuda.set_device(gpu)\n",
        "    set_seeds(seed)\n",
        "\n",
        "    train_loader, determin_train_loader, val_loader, test_loaders = get_data_loader()\n",
        "\n",
        "    # Define the neural network model\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.f_net = ResNet50_F(pretrained=True)\n",
        "            self.c_net_1 = ResNet50_C(pretrained=True)\n",
        "            self.c_net_2 = nn.Linear(self.f_net.output_dim, class_num)\n",
        "            self.c_net_2.weight.data.normal_(0, 0.01)\n",
        "            self.c_net_2.bias.data.fill_(0.0)\n",
        "\n",
        "        def forward(self, x):\n",
        "            feature = self.f_net(x)\n",
        "            out_1 = self.c_net_1(feature)\n",
        "            out_2 = self.c_net_2(feature)\n",
        "\n",
        "            return out_1, out_2\n",
        "\n",
        "    net = Net().cuda()\n",
        "\n",
        "    if os.path.exists(relationship_path):\n",
        "        print('Loading pre-computed relationship from {}.'.format(relationship_path))\n",
        "        relationship = np.load(relationship_path)\n",
        "    else:\n",
        "        print('Computing relationship')\n",
        "\n",
        "        def get_feature(loader):\n",
        "            train_labels_list = []\n",
        "            imagenet_labels_list = []\n",
        "\n",
        "            for train_inputs, train_labels in tqdm(loader):\n",
        "                net.eval()\n",
        "                train_labels_list.append(train_labels)\n",
        "\n",
        "                train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
        "                imagenet_labels, _ = net(train_inputs)\n",
        "                imagenet_labels = imagenet_labels.detach().cpu().numpy()\n",
        "\n",
        "                imagenet_labels_list.append(imagenet_labels)\n",
        "\n",
        "            all_train_labels = np.concatenate(train_labels_list, 0)\n",
        "            all_imagenet_labels = np.concatenate(imagenet_labels_list, 0)\n",
        "\n",
        "            return all_imagenet_labels, all_train_labels\n",
        "\n",
        "        train_imagenet_labels, train_train_labels = get_feature(determin_train_loader)\n",
        "        val_imagenet_labels, val_train_labels = get_feature(val_loader)\n",
        "        relationship = relationship_learning(train_imagenet_labels, train_train_labels,\n",
        "                                             val_imagenet_labels, val_train_labels)\n",
        "\n",
        "        np.save(relationship_path, relationship)\n",
        "\n",
        "    train(train_loader, val_loader, test_loaders, net, relationship)\n"
      ],
      "metadata": {
        "id": "eBAm_HtIT5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(train_loader, val_loader, test_loaders, net, relationship):\n",
        "    # Get the length of the train loader\n",
        "    train_len = len(train_loader) - 1\n",
        "    train_iter = iter(train_loader)\n",
        "\n",
        "    # Different learning rates for different layers\n",
        "    params_list = [\n",
        "        {\"params\": filter(lambda p: p.requires_grad, net.f_net.parameters())},\n",
        "        {\"params\": filter(lambda p: p.requires_grad, net.c_net_1.parameters())},\n",
        "        {\"params\": filter(lambda p: p.requires_grad, net.c_net_2.parameters()), \"lr\": lr * 10}\n",
        "    ]\n",
        "\n",
        "    # Optimizer setup\n",
        "    optimizer = torch.optim.SGD(params_list, lr=lr, weight_decay=weight_decay,\n",
        "                                momentum=momentum, nesterov=nesterov)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    milestones = [6000]\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)\n",
        "\n",
        "    # Check visual path\n",
        "    visual_path = os.path.join(visual_dir, name)\n",
        "    if not os.path.exists(visual_path):\n",
        "        os.makedirs(visual_path)\n",
        "    writer = get_writer(visual_path)\n",
        "\n",
        "    # Check model save path\n",
        "    save_path = os.path.join(save_dir, name)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for iter_num in range(total_iter):\n",
        "        net.train()\n",
        "\n",
        "        if iter_num % train_len == 0:\n",
        "            train_iter = iter(train_loader)\n",
        "\n",
        "        # Data Stage\n",
        "        data_start = time()\n",
        "\n",
        "        # Get the next batch of training inputs and labels\n",
        "        train_inputs, train_labels = next(train_iter)\n",
        "\n",
        "        # Convert labels to one-hot encoding and move data to GPU\n",
        "        imagenet_targets = torch.from_numpy(relationship[train_labels]).cuda().float()\n",
        "        train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
        "\n",
        "        data_duration = time() - data_start\n",
        "\n",
        "        # Calc Stage\n",
        "        calc_start = time()\n",
        "\n",
        "        # Forward pass\n",
        "        imagenet_outputs, train_outputs = net(train_inputs)\n",
        "\n",
        "        # Calculate losses\n",
        "        ce_loss = nn.CrossEntropyLoss()(train_outputs, train_labels)\n",
        "        imagenet_loss = - imagenet_targets * nn.LogSoftmax(dim=-1)(imagenet_outputs)\n",
        "        imagenet_loss = torch.mean(torch.sum(imagenet_loss, dim=-1))\n",
        "        loss = ce_loss + trade_off * imagenet_loss\n",
        "\n",
        "        # Log losses\n",
        "        writer.add_scalar('loss/ce_loss', ce_loss, iter_num)\n",
        "        writer.add_scalar('loss/imagenet_loss', imagenet_loss, iter_num)\n",
        "        writer.add_scalar('loss/loss', loss, iter_num)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        net.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        calc_duration = time() - calc_start\n",
        "\n",
        "        if iter_num % eval_iter == 0:\n",
        "            # Evaluation on validation dataset\n",
        "            acc_meter = AccuracyMeter(topk=(1,))\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                for val_inputs, val_labels in tqdm(val_loader):\n",
        "                    val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
        "                    _, val_outputs = net(val_inputs)\n",
        "                    acc_meter.update(val_outputs, val_labels)\n",
        "                writer.add_scalar('acc/val_acc', acc_meter.avg[1], iter_num)\n",
        "                print(\"Iter: {}/{} Val_Acc: {:2f}\".format(iter_num, total_iter, acc_meter.avg[1]))\n",
        "            acc_meter.reset()\n",
        "\n",
        "        if iter_num % save_iter == 0 and iter_num > 0:\n",
        "            # Evaluation on test dataset and model saving\n",
        "            test_acc = TenCropsTest(test_loaders, net)\n",
        "            writer.add_scalar('acc/test_acc', test_acc, iter_num)\n",
        "            print(\"Iter: {}/{} Test_Acc: {:2f}\".format(iter_num, total_iter, test_acc))\n",
        "            checkpoint = {\n",
        "                'state_dict': net.state_dict(),\n",
        "                'iter': iter_num,\n",
        "                'acc': test_acc,\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(save_path, '{}.pkl'.format(iter_num)))\n",
        "            print(\"Model Saved.\")\n",
        "\n",
        "        if iter_num % print_iter == 0:\n",
        "            # Print progress\n",
        "            print(\"Iter: {}/{} Loss_main: {:2f}, d/c: {}/{}\".format(iter_num, total_iter, loss,\n",
        "                                                                     data_duration, calc_duration))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c94HLIjdT6sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # print(\"PyTorch {}\".format(torch.__version__))\n",
        "    # print(\"TorchVision {}\".format(torchvision.__version__))\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "taWnESuzT7Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9a0210-6096-4517-f451-b627979a4eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 236MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-computed relationship from relationship.npy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [27:30<00:00,  6.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0/9050 Val_Acc: 0.567412\n",
            "Iter: 0/9050 Loss_main: 21.235962, d/c: 3.4217073917388916/2.002387285232544\n",
            "Iter: 100/9050 Loss_main: 10.397576, d/c: 0.4138517379760742/0.16207289695739746\n",
            "Iter: 200/9050 Loss_main: 10.094559, d/c: 0.31432652473449707/0.1681227684020996\n",
            "Iter: 300/9050 Loss_main: 8.873284, d/c: 0.32572054862976074/0.1576075553894043\n",
            "Iter: 400/9050 Loss_main: 7.709411, d/c: 0.281203031539917/0.1657414436340332\n",
            "Iter: 500/9050 Loss_main: 8.347646, d/c: 0.9492826461791992/0.1599268913269043\n",
            "Iter: 600/9050 Loss_main: 7.565969, d/c: 0.7830579280853271/0.16496729850769043\n",
            "Iter: 700/9050 Loss_main: 6.670202, d/c: 0.9346919059753418/0.16289162635803223\n",
            "Iter: 800/9050 Loss_main: 6.389672, d/c: 0.6015768051147461/0.18138527870178223\n",
            "Iter: 900/9050 Loss_main: 8.533734, d/c: 0.2918531894683838/0.18207359313964844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 1000/9050 Val_Acc: 81.444771\n",
            "Iter: 1000/9050 Loss_main: 6.877843, d/c: 0.3873310089111328/0.16183996200561523\n",
            "Iter: 1100/9050 Loss_main: 7.306240, d/c: 0.31978559494018555/0.17606353759765625\n",
            "Iter: 1200/9050 Loss_main: 6.164840, d/c: 0.3230776786804199/0.16017460823059082\n",
            "Iter: 1300/9050 Loss_main: 6.651655, d/c: 0.3894014358520508/0.15588617324829102\n",
            "Iter: 1400/9050 Loss_main: 6.455770, d/c: 0.3228952884674072/0.16415834426879883\n",
            "Iter: 1500/9050 Loss_main: 5.444801, d/c: 0.31731653213500977/0.173661470413208\n",
            "Iter: 1600/9050 Loss_main: 5.659955, d/c: 0.31207275390625/0.17258024215698242\n",
            "Iter: 1700/9050 Loss_main: 5.646506, d/c: 0.3158912658691406/0.1657419204711914\n",
            "Iter: 1800/9050 Loss_main: 6.001375, d/c: 0.31704068183898926/0.16053462028503418\n",
            "Iter: 1900/9050 Loss_main: 5.557127, d/c: 0.32526373863220215/0.15932917594909668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 2000/9050 Val_Acc: 89.456291\n",
            "Iter: 2000/9050 Loss_main: 6.039419, d/c: 0.3634054660797119/0.15803766250610352\n",
            "Iter: 2100/9050 Loss_main: 5.466941, d/c: 0.31296420097351074/0.15680360794067383\n",
            "Iter: 2200/9050 Loss_main: 5.732442, d/c: 0.3184378147125244/0.15892767906188965\n",
            "Iter: 2300/9050 Loss_main: 6.107769, d/c: 0.3145103454589844/0.17448925971984863\n",
            "Iter: 2400/9050 Loss_main: 5.956676, d/c: 0.30258703231811523/0.16739797592163086\n",
            "Iter: 2500/9050 Loss_main: 5.792704, d/c: 0.31651878356933594/0.18184328079223633\n",
            "Iter: 2600/9050 Loss_main: 6.137660, d/c: 0.3307063579559326/0.16489601135253906\n",
            "Iter: 2700/9050 Loss_main: 5.180362, d/c: 0.3211195468902588/0.16051435470581055\n",
            "Iter: 2800/9050 Loss_main: 5.676848, d/c: 0.32062458992004395/0.16235852241516113\n",
            "Iter: 2900/9050 Loss_main: 5.750184, d/c: 0.31626462936401367/0.17041587829589844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:51<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 3000/9050 Val_Acc: 92.894630\n",
            "Iter: 3000/9050 Loss_main: 5.359901, d/c: 0.3238992691040039/0.1619882583618164\n",
            "Iter: 3100/9050 Loss_main: 5.341998, d/c: 0.3881337642669678/0.16286969184875488\n",
            "Iter: 3200/9050 Loss_main: 5.225932, d/c: 0.322908878326416/0.15955519676208496\n",
            "Iter: 3300/9050 Loss_main: 5.147526, d/c: 0.3085916042327881/0.15896344184875488\n",
            "Iter: 3400/9050 Loss_main: 5.163166, d/c: 0.38439154624938965/0.16325950622558594\n",
            "Iter: 3500/9050 Loss_main: 5.430894, d/c: 0.38226318359375/0.15519452095031738\n",
            "Iter: 3600/9050 Loss_main: 5.139458, d/c: 0.3388864994049072/0.1697087287902832\n",
            "Iter: 3700/9050 Loss_main: 5.042335, d/c: 0.3273146152496338/0.16856813430786133\n",
            "Iter: 3800/9050 Loss_main: 4.970862, d/c: 0.4395787715911865/0.18523502349853516\n",
            "Iter: 3900/9050 Loss_main: 5.907378, d/c: 0.3206300735473633/0.1696491241455078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:45<00:00,  2.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 4000/9050 Val_Acc: 95.689316\n",
            "Iter: 4000/9050 Loss_main: 5.582864, d/c: 0.32436275482177734/0.17222356796264648\n",
            "Iter: 4100/9050 Loss_main: 5.909525, d/c: 0.2839982509613037/0.16730046272277832\n",
            "Iter: 4200/9050 Loss_main: 5.159949, d/c: 0.32003092765808105/0.16126275062561035\n",
            "Iter: 4300/9050 Loss_main: 5.054413, d/c: 0.3144063949584961/0.16941452026367188\n",
            "Iter: 4400/9050 Loss_main: 5.280721, d/c: 0.3868393898010254/0.16321372985839844\n",
            "Iter: 4500/9050 Loss_main: 5.238613, d/c: 0.3203921318054199/0.16097164154052734\n",
            "Iter: 4600/9050 Loss_main: 5.381647, d/c: 0.3260807991027832/0.15623736381530762\n",
            "Iter: 4700/9050 Loss_main: 5.555718, d/c: 0.3235292434692383/0.17194771766662598\n",
            "Iter: 4800/9050 Loss_main: 5.549509, d/c: 0.30980968475341797/0.16274237632751465\n",
            "Iter: 4900/9050 Loss_main: 5.005818, d/c: 0.858701229095459/0.16877269744873047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 5000/9050 Val_Acc: 97.264542\n",
            "Iter: 5000/9050 Loss_main: 4.394911, d/c: 0.32352471351623535/0.15900063514709473\n",
            "Iter: 5100/9050 Loss_main: 4.865605, d/c: 0.3183915615081787/0.1756131649017334\n",
            "Iter: 5200/9050 Loss_main: 5.153826, d/c: 0.29007458686828613/0.19354677200317383\n",
            "Iter: 5300/9050 Loss_main: 5.716899, d/c: 0.31125712394714355/0.1813793182373047\n",
            "Iter: 5400/9050 Loss_main: 5.267262, d/c: 0.41721105575561523/0.15568089485168457\n",
            "Iter: 5500/9050 Loss_main: 5.276050, d/c: 0.38530969619750977/0.1668083667755127\n",
            "Iter: 5600/9050 Loss_main: 5.701625, d/c: 0.3675856590270996/0.17180728912353516\n",
            "Iter: 5700/9050 Loss_main: 4.915949, d/c: 0.3243408203125/0.159621000289917\n",
            "Iter: 5800/9050 Loss_main: 4.250974, d/c: 0.325425386428833/0.15685343742370605\n",
            "Iter: 5900/9050 Loss_main: 4.800254, d/c: 0.3199188709259033/0.16908931732177734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:48<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 6000/9050 Val_Acc: 98.213066\n",
            "Iter: 6000/9050 Loss_main: 5.897944, d/c: 0.3291194438934326/0.15900349617004395\n",
            "Iter: 6100/9050 Loss_main: 5.121997, d/c: 0.3177022933959961/0.16529512405395508\n",
            "Iter: 6200/9050 Loss_main: 5.690584, d/c: 0.3198823928833008/0.17158150672912598\n",
            "Iter: 6300/9050 Loss_main: 4.567341, d/c: 0.32100486755371094/0.16512393951416016\n",
            "Iter: 6400/9050 Loss_main: 4.530938, d/c: 0.32823824882507324/0.17086124420166016\n",
            "Iter: 6500/9050 Loss_main: 4.508157, d/c: 0.3162879943847656/0.16156625747680664\n",
            "Iter: 6600/9050 Loss_main: 4.533612, d/c: 0.3212578296661377/0.165177583694458\n",
            "Iter: 6700/9050 Loss_main: 4.162060, d/c: 0.3186061382293701/0.1585071086883545\n",
            "Iter: 6800/9050 Loss_main: 4.828248, d/c: 0.3116183280944824/0.16470623016357422\n",
            "Iter: 6900/9050 Loss_main: 5.357476, d/c: 0.3195338249206543/0.1825113296508789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:48<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 7000/9050 Val_Acc: 99.254723\n",
            "Iter: 7000/9050 Loss_main: 4.490765, d/c: 0.32824110984802246/0.1679680347442627\n",
            "Iter: 7100/9050 Loss_main: 5.117365, d/c: 0.29003190994262695/0.1804969310760498\n",
            "Iter: 7200/9050 Loss_main: 5.553283, d/c: 0.28574490547180176/0.17293787002563477\n",
            "Iter: 7300/9050 Loss_main: 4.617667, d/c: 0.3139469623565674/0.16779756546020508\n",
            "Iter: 7400/9050 Loss_main: 5.272613, d/c: 0.5334460735321045/0.1760408878326416\n",
            "Iter: 7500/9050 Loss_main: 4.719364, d/c: 0.3244597911834717/0.18225312232971191\n",
            "Iter: 7600/9050 Loss_main: 5.624404, d/c: 0.293698787689209/0.1625068187713623\n",
            "Iter: 7700/9050 Loss_main: 4.771202, d/c: 0.780968189239502/0.17334842681884766\n",
            "Iter: 7800/9050 Loss_main: 4.815221, d/c: 0.3237295150756836/0.1734921932220459\n",
            "Iter: 7900/9050 Loss_main: 5.435906, d/c: 0.38922715187072754/0.1680591106414795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:51<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 8000/9050 Val_Acc: 99.432549\n",
            "Iter: 8000/9050 Loss_main: 4.983940, d/c: 0.32604432106018066/0.16464447975158691\n",
            "Iter: 8100/9050 Loss_main: 4.442134, d/c: 0.31666088104248047/0.1868443489074707\n",
            "Iter: 8200/9050 Loss_main: 5.121646, d/c: 0.31320905685424805/0.15841269493103027\n",
            "Iter: 8300/9050 Loss_main: 4.692663, d/c: 0.31890106201171875/0.16425609588623047\n",
            "Iter: 8400/9050 Loss_main: 5.353915, d/c: 0.32420778274536133/0.16029572486877441\n",
            "Iter: 8500/9050 Loss_main: 4.749154, d/c: 0.31821680068969727/0.1712348461151123\n",
            "Iter: 8600/9050 Loss_main: 5.148423, d/c: 0.31516361236572266/0.16322660446166992\n",
            "Iter: 8700/9050 Loss_main: 4.427621, d/c: 0.32207775115966797/0.15943598747253418\n",
            "Iter: 8800/9050 Loss_main: 4.771709, d/c: 0.31677961349487305/0.1577284336090088\n",
            "Iter: 8900/9050 Loss_main: 4.423179, d/c: 0.314708948135376/0.15876555442810059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:51<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 9000/9050 Val_Acc: 99.542664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2947/2947 [21:24<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 9000/9050 Test_Acc: 99.728539\n",
            "Model Saved.\n",
            "Iter: 9000/9050 Loss_main: 4.694311, d/c: 0.31838011741638184/0.15957331657409668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd r\"/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils/model/StochNorm/9000.pkl\"\n",
        "# %cd r\"/content/drive/MyDrive/Colab Notebooks/CoTuning-main/testing.jpg\""
      ],
      "metadata": {
        "id": "duWp-0Xdmnhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9e75f5-95ba-4538-97ac-eee6955be94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'r/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils/model/StochNorm/9000.pkl'\n",
            "/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils\n",
            "[Errno 2] No such file or directory: 'r/content/drive/MyDrive/Colab Notebooks/CoTuning-main/testing.jpg'\n",
            "/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# # Load the trained model checkpoint\n",
        "# checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils/model/StochNorm/9000.pkl'\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "# net = Net()\n",
        "# net.load_state_dict(checkpoint['state_dict'])\n",
        "# net.eval()\n",
        "\n",
        "# # Define the image transformation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Load and preprocess the image\n",
        "# image_path = '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/testing.jpg'\n",
        "# image = Image.open(image_path).convert('RGB')\n",
        "# image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "# # Perform the prediction\n",
        "# with torch.no_grad():\n",
        "#     outputs = net(image_tensor)\n",
        "\n",
        "# # Process the prediction results\n",
        "# _, predicted = torch.max(outputs[1], 1)\n",
        "# class_index = predicted.item()\n",
        "\n",
        "# # Load the class labels\n",
        "# class_labels_path = '/path/to/your/class_labels.txt'\n",
        "# with open(class_labels_path, 'r') as f:\n",
        "#     class_labels = f.readlines()\n",
        "# class_labels = [label.strip() for label in class_labels]\n",
        "\n",
        "# # Get the predicted class label\n",
        "# predicted_label = class_labels[class_index]\n",
        "\n",
        "# # Print the predicted label\n",
        "# print(\"Predicted class label:\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "XQa5amDgrVuT",
        "outputId": "49153130-f12f-49f8-8f60-ddf96ad4a93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-bb64ce1314b5>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Load the class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mclass_labels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/path/to/your/class_labels.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mclass_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/class_labels.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage\n",
        "# image_path = '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/testing.jpg'  # Replace with the path to your image\n",
        "# model_path = '/content/drive/MyDrive/Colab Notebooks/CoTuning-main/utils/model/StochNorm/9000.pkl'  # Replace with the path to your saved model\n"
      ],
      "metadata": {
        "id": "nxm6BByEw2xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import datasets\n",
        "# from tqdm import tqdm\n",
        "# from PIL import Image\n",
        "\n",
        "# # from module.backbone import ResNet50_F, ResNet50_C\n",
        "# # from module.relationship_learning import relationship_learning\n",
        "# # from utils.transforms import get_transforms\n",
        "# # from utils.tools import TenCropsTest\n",
        "\n",
        "\n",
        "# def get_data_loader():\n",
        "#     data_transforms = get_transforms(resize_size=256, crop_size=224)\n",
        "\n",
        "#     # build dataset\n",
        "#     test_datasets = {\n",
        "#         'test' + str(i):\n",
        "#             datasets.ImageFolder(\n",
        "#                 os.path.join(data_path, 'images'),\n",
        "#                 transform=data_transforms[\"test\" + str(i)]\n",
        "#         )\n",
        "#         for i in range(10)\n",
        "#     }\n",
        "\n",
        "#     # build dataloader\n",
        "#     test_loaders = {\n",
        "#         'test' + str(i):\n",
        "#             DataLoader(\n",
        "#                 test_datasets[\"test\" + str(i)],\n",
        "#                 batch_size=1, shuffle=False, num_workers=num_workers\n",
        "#         )\n",
        "#         for i in range(10)\n",
        "#     }\n",
        "\n",
        "#     return test_loaders\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     # configs = get_configs()\n",
        "#     # print(configs)\n",
        "\n",
        "#     device = torch.device('cpu')\n",
        "\n",
        "#     net = Net().to(device)\n",
        "#     if os.path.exists(relationship_path):\n",
        "#         print('load pre-computed relationship from {}.'.format(relationship_path))\n",
        "#         relationship = np.load(relationship_path)\n",
        "#     else:\n",
        "#         print('computing relationship')\n",
        "#         # ... relationship computation code ...\n",
        "\n",
        "#     test_loaders = get_data_loader()\n",
        "#     checkpoint_path = '/path/to/your/model.pkl'"
      ],
      "metadata": {
        "id": "JCcvEqSjgvZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_z_RbOcEC2zT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}